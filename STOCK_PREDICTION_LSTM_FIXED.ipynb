{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improved Stock Price Prediction using LSTM\n",
        "\n",
        "This notebook provides a fixed and improved version of the stock price prediction model using LSTM neural networks. All the issues from the original version have been corrected.\n",
        "\n",
        "## Key Improvements:\n",
        "- Fixed variable scope issues\n",
        "- Proper data preprocessing\n",
        "- Complete LSTM model implementation\n",
        "- Enhanced technical indicators\n",
        "- Better error handling\n",
        "- Comprehensive evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yfinance as yf\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# TensorFlow and Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch stock data\n",
        "def fetch_stock_data(symbols, years=3):\n",
        "    \"\"\"\n",
        "    Fetch stock data for multiple symbols.\n",
        "    \"\"\"\n",
        "    print(\"Fetching stock data...\")\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=years*365)\n",
        "    \n",
        "    stock_data = {}\n",
        "    company_names = {\n",
        "        'AAPL': 'Apple Inc.',\n",
        "        'GOOGL': 'Alphabet Inc.',\n",
        "        'MSFT': 'Microsoft Corporation',\n",
        "        'AMZN': 'Amazon.com Inc.',\n",
        "        'TSLA': 'Tesla Inc.',\n",
        "        'META': 'Meta Platforms Inc.',\n",
        "        'NVDA': 'NVIDIA Corporation'\n",
        "    }\n",
        "    \n",
        "    for symbol in symbols:\n",
        "        try:\n",
        "            print(f\"Fetching data for {symbol}...\")\n",
        "            data = yf.download(symbol, start=start_date, end=end_date)\n",
        "            if not data.empty:\n",
        "                data['Symbol'] = symbol\n",
        "                data['Company'] = company_names.get(symbol, symbol)\n",
        "                stock_data[symbol] = data\n",
        "                print(f\"Successfully fetched {len(data)} records for {symbol}\")\n",
        "            else:\n",
        "                print(f\"Warning: No data found for {symbol}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for {symbol}: {str(e)}\")\n",
        "            \n",
        "    return stock_data\n",
        "\n",
        "# List of stocks to analyze\n",
        "stocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\n",
        "\n",
        "# Fetch the data\n",
        "stock_data = fetch_stock_data(stocks, years=3)\n",
        "\n",
        "# Display basic info about the fetched data\n",
        "for symbol, data in stock_data.items():\n",
        "    print(f\"\\n{symbol} data shape: {data.shape}\")\n",
        "    print(f\"Date range: {data.index.min().date()} to {data.index.max().date()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the data for Apple (AAPL) as an example\n",
        "if 'AAPL' in stock_data:\n",
        "    print(\"Apple Stock Data (First 5 rows):\")\n",
        "    print(stock_data['AAPL'].head())\n",
        "    \n",
        "    print(\"\\nApple Stock Data (Last 5 rows):\")\n",
        "    print(stock_data['AAPL'].tail())\n",
        "    \n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(stock_data['AAPL'][['Open', 'High', 'Low', 'Close', 'Volume']].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_technical_indicators(data):\n",
        "    \"\"\"\n",
        "    Add technical indicators to the stock data.\n",
        "    \"\"\"\n",
        "    df = data.copy()\n",
        "    \n",
        "    # Moving averages\n",
        "    df['MA_7'] = df['Close'].rolling(window=7).mean()\n",
        "    df['MA_21'] = df['Close'].rolling(window=21).mean()\n",
        "    df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
        "    \n",
        "    # Exponential moving averages\n",
        "    df['EMA_12'] = df['Close'].ewm(span=12).mean()\n",
        "    df['EMA_26'] = df['Close'].ewm(span=26).mean()\n",
        "    \n",
        "    # MACD\n",
        "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
        "    df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "    \n",
        "    # RSI\n",
        "    delta = df['Close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "    \n",
        "    # Bollinger Bands\n",
        "    df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "    bb_std = df['Close'].rolling(window=20).std()\n",
        "    df['BB_Upper'] = df['BB_Middle'] + (bb_std * 2)\n",
        "    df['BB_Lower'] = df['BB_Middle'] - (bb_std * 2)\n",
        "    df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
        "    df['BB_Position'] = (df['Close'] - df['BB_Lower']) / df['BB_Width']\n",
        "    \n",
        "    # Volume indicators\n",
        "    df['Volume_MA'] = df['Volume'].rolling(window=20).mean()\n",
        "    df['Volume_Ratio'] = df['Volume'] / df['Volume_MA']\n",
        "    \n",
        "    # Price features\n",
        "    df['High_Low_Pct'] = (df['High'] - df['Low']) / df['Close'] * 100\n",
        "    df['Price_Change'] = df['Close'].pct_change()\n",
        "    df['Price_Range'] = df['High'] - df['Low']\n",
        "    \n",
        "    # Volatility\n",
        "    df['Volatility'] = df['Price_Change'].rolling(window=20).std()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Add technical indicators to Apple data\n",
        "if 'AAPL' in stock_data:\n",
        "    aapl_enhanced = add_technical_indicators(stock_data['AAPL'])\n",
        "    print(\"Enhanced Apple data with technical indicators:\")\n",
        "    print(f\"Shape: {aapl_enhanced.shape}\")\n",
        "    print(\"\\nNew columns added:\")\n",
        "    new_columns = [col for col in aapl_enhanced.columns if col not in stock_data['AAPL'].columns]\n",
        "    print(new_columns)\n",
        "    \n",
        "    # Display sample data\n",
        "    print(\"\\nSample technical indicators (last 5 rows):\")\n",
        "    print(aapl_enhanced[['Close', 'MA_7', 'MA_21', 'RSI', 'MACD', 'BB_Position']].tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the stock data with moving averages\n",
        "if 'AAPL' in stock_data:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Plot 1: Stock price with moving averages\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(aapl_enhanced.index, aapl_enhanced['Close'], label='Close Price', linewidth=2)\n",
        "    ax1.plot(aapl_enhanced.index, aapl_enhanced['MA_7'], label='7-day MA', alpha=0.7)\n",
        "    ax1.plot(aapl_enhanced.index, aapl_enhanced['MA_21'], label='21-day MA', alpha=0.7)\n",
        "    ax1.plot(aapl_enhanced.index, aapl_enhanced['MA_50'], label='50-day MA', alpha=0.7)\n",
        "    ax1.set_title('AAPL Stock Price with Moving Averages')\n",
        "    ax1.set_xlabel('Date')\n",
        "    ax1.set_ylabel('Price ($)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: RSI\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot(aapl_enhanced.index, aapl_enhanced['RSI'], label='RSI', color='orange')\n",
        "    ax2.axhline(y=70, color='r', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
        "    ax2.axhline(y=30, color='g', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
        "    ax2.set_title('RSI (Relative Strength Index)')\n",
        "    ax2.set_xlabel('Date')\n",
        "    ax2.set_ylabel('RSI')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: MACD\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.plot(aapl_enhanced.index, aapl_enhanced['MACD'], label='MACD', color='blue')\n",
        "    ax3.plot(aapl_enhanced.index, aapl_enhanced['MACD_Signal'], label='Signal', color='red')\n",
        "    ax3.bar(aapl_enhanced.index, aapl_enhanced['MACD_Histogram'], label='Histogram', alpha=0.3, color='green')\n",
        "    ax3.set_title('MACD')\n",
        "    ax3.set_xlabel('Date')\n",
        "    ax3.set_ylabel('MACD')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Volume\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.bar(aapl_enhanced.index, aapl_enhanced['Volume'], alpha=0.7, color='purple')\n",
        "    ax4.plot(aapl_enhanced.index, aapl_enhanced['Volume_MA'], label='Volume MA', color='red', linewidth=2)\n",
        "    ax4.set_title('Volume with Moving Average')\n",
        "    ax4.set_xlabel('Date')\n",
        "    ax4.set_ylabel('Volume')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_lstm_data(data, lookback_window=60, test_size=0.2, target_column='Close'):\n",
        "    \"\"\"\n",
        "    Prepare data for LSTM model training.\n",
        "    \"\"\"\n",
        "    print(f\"Preparing LSTM data with lookback window: {lookback_window}\")\n",
        "    \n",
        "    # Add technical indicators\n",
        "    enhanced_data = add_technical_indicators(data)\n",
        "    \n",
        "    # Select features for prediction\n",
        "    feature_columns = [\n",
        "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "        'MA_7', 'MA_21', 'MA_50', 'EMA_12', 'EMA_26',\n",
        "        'MACD', 'MACD_Signal', 'RSI', 'BB_Position',\n",
        "        'Volume_Ratio', 'High_Low_Pct', 'Volatility'\n",
        "    ]\n",
        "    \n",
        "    # Remove columns with too many NaN values\n",
        "    available_columns = []\n",
        "    for col in feature_columns:\n",
        "        if col in enhanced_data.columns:\n",
        "            # Check if column has enough valid data\n",
        "            valid_ratio = enhanced_data[col].notna().sum() / len(enhanced_data)\n",
        "            if valid_ratio > 0.8:  # At least 80% valid data\n",
        "                available_columns.append(col)\n",
        "    \n",
        "    print(f\"Using {len(available_columns)} features: {available_columns}\")\n",
        "    \n",
        "    # Create dataset with selected features\n",
        "    dataset = enhanced_data[available_columns].copy()\n",
        "    \n",
        "    # Forward fill missing values and then drop remaining NaN\n",
        "    dataset = dataset.fillna(method='ffill').dropna()\n",
        "    \n",
        "    if len(dataset) < lookback_window + 50:  # Need minimum data\n",
        "        raise ValueError(f\"Not enough data after cleaning. Got {len(dataset)} rows, need at least {lookback_window + 50}\")\n",
        "    \n",
        "    print(f\"Dataset shape after cleaning: {dataset.shape}\")\n",
        "    \n",
        "    # Scale the data\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(dataset.values)\n",
        "    \n",
        "    # Create sequences for LSTM\n",
        "    X, y = [], []\n",
        "    target_idx = available_columns.index(target_column)\n",
        "    \n",
        "    for i in range(lookback_window, len(scaled_data)):\n",
        "        X.append(scaled_data[i-lookback_window:i])\n",
        "        y.append(scaled_data[i, target_idx])\n",
        "    \n",
        "    X, y = np.array(X), np.array(y)\n",
        "    \n",
        "    # Split into train and test sets\n",
        "    split_idx = int(len(X) * (1 - test_size))\n",
        "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "    \n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Testing data shape: {X_test.shape}\")\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test, scaler, available_columns, dataset\n",
        "\n",
        "# Prepare the data\n",
        "if 'AAPL' in stock_data:\n",
        "    try:\n",
        "        X_train, X_test, y_train, y_test, scaler, feature_columns, dataset = prepare_lstm_data(\n",
        "            stock_data['AAPL'], \n",
        "            lookback_window=60, \n",
        "            test_size=0.2\n",
        "        )\n",
        "        print(\"\\nData preparation completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in data preparation: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lstm_model(input_shape, lstm_units=[50, 50, 50], dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Build and compile the LSTM model.\n",
        "    \"\"\"\n",
        "    print(\"Building LSTM model...\")\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # First LSTM layer\n",
        "    model.add(LSTM(units=lstm_units[0], \n",
        "                  return_sequences=True, \n",
        "                  input_shape=input_shape))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(BatchNormalization())\n",
        "    \n",
        "    # Additional LSTM layers\n",
        "    for i in range(1, len(lstm_units)):\n",
        "        return_sequences = i < len(lstm_units) - 1\n",
        "        model.add(LSTM(units=lstm_units[i], return_sequences=return_sequences))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "        if return_sequences:\n",
        "            model.add(BatchNormalization())\n",
        "    \n",
        "    # Dense layers\n",
        "    model.add(Dense(25, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate/2))\n",
        "    model.add(Dense(1))\n",
        "    \n",
        "    # Compile model\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, \n",
        "                 loss='mean_squared_error',\n",
        "                 metrics=['mae'])\n",
        "    \n",
        "    print(f\"Model built with {model.count_params():,} parameters\")\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "if 'X_train' in locals():\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    model = build_lstm_model(\n",
        "        input_shape=input_shape,\n",
        "        lstm_units=[64, 32, 16],\n",
        "        dropout_rate=0.2,\n",
        "        learning_rate=0.001\n",
        "    )\n",
        "else:\n",
        "    print(\"Training data not available. Please run the data preparation cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "if 'model' in locals() and 'X_train' in locals():\n",
        "    print(\"Training LSTM model...\")\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7),\n",
        "        ModelCheckpoint('best_stock_model.h5', monitor='val_loss', save_best_only=True)\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=50,  # Reduced for demonstration\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"\\nTraining completed!\")\n",
        "    \n",
        "    # Plot training history\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Loss plot\n",
        "    axes[0].plot(history.history['loss'], label='Training Loss')\n",
        "    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_title('Model Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # MAE plot\n",
        "    axes[1].plot(history.history['mae'], label='Training MAE')\n",
        "    axes[1].plot(history.history['val_mae'], label='Validation MAE')\n",
        "    axes[1].set_title('Model MAE')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Model or training data not available. Please run previous cells first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions and evaluate the model\n",
        "if 'model' in locals() and 'X_test' in locals():\n",
        "    print(\"Making predictions...\")\n",
        "    \n",
        "    # Make predictions\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Inverse transform predictions (only for the target column)\n",
        "    # Create dummy arrays with the right shape for inverse transform\n",
        "    dummy_train = np.zeros((train_pred.shape[0], len(feature_columns)))\n",
        "    dummy_test = np.zeros((test_pred.shape[0], len(feature_columns)))\n",
        "    \n",
        "    # Place predictions in the target column position\n",
        "    target_idx = feature_columns.index('Close')\n",
        "    dummy_train[:, target_idx] = train_pred.flatten()\n",
        "    dummy_test[:, target_idx] = test_pred.flatten()\n",
        "    \n",
        "    # Inverse transform\n",
        "    train_pred_scaled = scaler.inverse_transform(dummy_train)[:, target_idx]\n",
        "    test_pred_scaled = scaler.inverse_transform(dummy_test)[:, target_idx]\n",
        "    \n",
        "    # Also inverse transform actual values\n",
        "    dummy_train_actual = np.zeros((len(y_train), len(feature_columns)))\n",
        "    dummy_test_actual = np.zeros((len(y_test), len(feature_columns)))\n",
        "    \n",
        "    dummy_train_actual[:, target_idx] = y_train\n",
        "    dummy_test_actual[:, target_idx] = y_test\n",
        "    \n",
        "    y_train_scaled = scaler.inverse_transform(dummy_train_actual)[:, target_idx]\n",
        "    y_test_scaled = scaler.inverse_transform(dummy_test_actual)[:, target_idx]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train_scaled, train_pred_scaled))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test_scaled, test_pred_scaled))\n",
        "    train_mae = mean_absolute_error(y_train_scaled, train_pred_scaled)\n",
        "    test_mae = mean_absolute_error(y_test_scaled, test_pred_scaled)\n",
        "    train_r2 = r2_score(y_train_scaled, train_pred_scaled)\n",
        "    test_r2 = r2_score(y_test_scaled, test_pred_scaled)\n",
        "    \n",
        "    # Calculate accuracy as percentage\n",
        "    test_accuracy = max(0, 100 - (test_rmse / np.mean(y_test_scaled) * 100))\n",
        "    \n",
        "    print(f\"\\n=== Model Evaluation Results ===\")\n",
        "    print(f\"Train RMSE: ${train_rmse:.2f}\")\n",
        "    print(f\"Test RMSE: ${test_rmse:.2f}\")\n",
        "    print(f\"Test MAE: ${test_mae:.2f}\")\n",
        "    print(f\"Test R²: {test_r2:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    \n",
        "    print(\"\\nPredictions completed successfully!\")\n",
        "else:\n",
        "    print(\"Model or test data not available. Please run previous cells first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "if 'test_pred_scaled' in locals():\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Plot 1: Training and Test Predictions\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.plot(y_train_scaled, label='Training Actual', alpha=0.7)\n",
        "    ax1.plot(train_pred_scaled, label='Training Predicted', alpha=0.7)\n",
        "    ax1.plot(range(len(y_train_scaled), len(y_train_scaled) + len(y_test_scaled)), \n",
        "            y_test_scaled, label='Test Actual', alpha=0.7)\n",
        "    ax1.plot(range(len(y_train_scaled), len(y_train_scaled) + len(test_pred_scaled)), \n",
        "            test_pred_scaled, label='Test Predicted', alpha=0.7)\n",
        "    ax1.set_title('AAPL Stock Price Prediction')\n",
        "    ax1.set_xlabel('Time')\n",
        "    ax1.set_ylabel('Price ($)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Test Set Zoom\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.plot(y_test_scaled, label='Actual', linewidth=2, color='blue')\n",
        "    ax2.plot(test_pred_scaled, label='Predicted', linewidth=2, alpha=0.8, color='red')\n",
        "    ax2.set_title('Test Set Predictions (Zoomed)')\n",
        "    ax2.set_xlabel('Time')\n",
        "    ax2.set_ylabel('Price ($)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Residuals\n",
        "    ax3 = axes[1, 0]\n",
        "    residuals = y_test_scaled - test_pred_scaled\n",
        "    ax3.scatter(test_pred_scaled, residuals, alpha=0.6)\n",
        "    ax3.axhline(y=0, color='r', linestyle='--')\n",
        "    ax3.set_title('Residual Plot')\n",
        "    ax3.set_xlabel('Predicted Price ($)')\n",
        "    ax3.set_ylabel('Residuals ($)')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Metrics Bar Chart\n",
        "    ax4 = axes[1, 1]\n",
        "    metric_names = ['RMSE', 'MAE', 'R²', 'Accuracy (%)']\n",
        "    metric_values = [test_rmse, test_mae, test_r2, test_accuracy]\n",
        "    \n",
        "    colors = ['red', 'orange', 'green', 'blue']\n",
        "    bars = ax4.bar(metric_names, metric_values, color=colors, alpha=0.7)\n",
        "    ax4.set_title('Model Performance Metrics')\n",
        "    ax4.set_ylabel('Value')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, metric_values):\n",
        "        height = bar.get_height()\n",
        "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{value:.2f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Additional residual analysis\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(residuals, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    plt.title('Residual Distribution')\n",
        "    plt.xlabel('Residual Value ($)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(residuals, color='purple', alpha=0.7)\n",
        "    plt.title('Residuals Over Time')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Residual Value ($)')\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Predictions not available. Please run the prediction cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comparison dataframe for easier analysis\n",
        "if 'test_pred_scaled' in locals():\n",
        "    # Create a dataframe with actual vs predicted values\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Actual': y_test_scaled,\n",
        "        'Predicted': test_pred_scaled,\n",
        "        'Residual': y_test_scaled - test_pred_scaled,\n",
        "        'Absolute_Error': np.abs(y_test_scaled - test_pred_scaled),\n",
        "        'Percentage_Error': np.abs((y_test_scaled - test_pred_scaled) / y_test_scaled) * 100\n",
        "    })\n",
        "    \n",
        "    print(\"=== Prediction Analysis ===\")\n",
        "    print(f\"\\nFirst 10 predictions:\")\n",
        "    print(comparison_df.head(10).round(2))\n",
        "    \n",
        "    print(f\"\\nLast 10 predictions:\")\n",
        "    print(comparison_df.tail(10).round(2))\n",
        "    \n",
        "    print(f\"\\nSummary Statistics:\")\n",
        "    print(comparison_df.describe().round(2))\n",
        "    \n",
        "    # Find best and worst predictions\n",
        "    best_prediction_idx = comparison_df['Absolute_Error'].idxmin()\n",
        "    worst_prediction_idx = comparison_df['Absolute_Error'].idxmax()\n",
        "    \n",
        "    print(f\"\\nBest Prediction (lowest absolute error):\")\n",
        "    print(f\"Actual: ${comparison_df.loc[best_prediction_idx, 'Actual']:.2f}\")\n",
        "    print(f\"Predicted: ${comparison_df.loc[best_prediction_idx, 'Predicted']:.2f}\")\n",
        "    print(f\"Error: ${comparison_df.loc[best_prediction_idx, 'Absolute_Error']:.2f}\")\n",
        "    \n",
        "    print(f\"\\nWorst Prediction (highest absolute error):\")\n",
        "    print(f\"Actual: ${comparison_df.loc[worst_prediction_idx, 'Actual']:.2f}\")\n",
        "    print(f\"Predicted: ${comparison_df.loc[worst_prediction_idx, 'Predicted']:.2f}\")\n",
        "    print(f\"Error: ${comparison_df.loc[worst_prediction_idx, 'Absolute_Error']:.2f}\")\n",
        "else:\n",
        "    print(\"Predictions not available. Please run the prediction cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model and scaler for future use\n",
        "if 'model' in locals() and 'scaler' in locals():\n",
        "    import joblib\n",
        "    \n",
        "    # Save the model\n",
        "    model.save('improved_stock_lstm_model.h5')\n",
        "    print(\"Model saved as 'improved_stock_lstm_model.h5'\")\n",
        "    \n",
        "    # Save the scaler and feature columns\n",
        "    model_data = {\n",
        "        'scaler': scaler,\n",
        "        'feature_columns': feature_columns,\n",
        "        'lookback_window': 60\n",
        "    }\n",
        "    joblib.dump(model_data, 'model_data.pkl')\n",
        "    print(\"Scaler and feature information saved as 'model_data.pkl'\")\n",
        "    \n",
        "    print(\"\\n=== Model Training and Evaluation Complete ===\")\n",
        "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"Final Test RMSE: ${test_rmse:.2f}\")\n",
        "    print(f\"Final Test R²: {test_r2:.4f}\")\n",
        "else:\n",
        "    print(\"Model or scaler not available for saving.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This improved notebook has successfully addressed all the issues from the original implementation:\n",
        "\n",
        "### Fixed Issues:\n",
        "1. **Variable Scope**: All variables are properly defined in the correct order\n",
        "2. **Missing Imports**: All required libraries are imported at the beginning\n",
        "3. **Data Preprocessing**: Enhanced with proper handling of NaN values and feature selection\n",
        "4. **Model Architecture**: Complete LSTM implementation with proper layers\n",
        "5. **Evaluation Metrics**: Comprehensive evaluation with multiple metrics\n",
        "6. **Error Handling**: Proper exception handling and data validation\n",
        "\n",
        "### Enhancements:\n",
        "1. **Technical Indicators**: Added RSI, MACD, Bollinger Bands, and volume indicators\n",
        "2. **Better Visualizations**: Multiple plots for comprehensive analysis\n",
        "3. **Model Callbacks**: Early stopping, learning rate reduction, and model checkpointing\n",
        "4. **Residual Analysis**: Detailed error analysis and distribution plots\n",
        "5. **Model Persistence**: Ability to save and load trained models\n",
        "6. **Comprehensive Metrics**: RMSE, MAE, R², and accuracy calculations\n",
        "\n",
        "### Usage:\n",
        "Run all cells in sequence to:\n",
        "1. Fetch stock data\n",
        "2. Add technical indicators\n",
        "3. Prepare data for LSTM\n",
        "4. Build and train the model\n",
        "5. Evaluate performance\n",
        "6. Visualize results\n",
        "7. Save the model for future use\n",
        "\n",
        "The model should now provide much better performance and reliability compared to the original implementation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
